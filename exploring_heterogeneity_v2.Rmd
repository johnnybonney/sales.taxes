---
title: "Exploring Heterogeneity (Sales Taxes, 2009-2013)"
author: "John Bonney"
date: "January 7, 2019"
output:
  pdf_document:
    fig_caption: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(kableExtra)
library(data.table)
library(ggplot2)
library(cowplot)
library(zoo)
library(tidyr)
```

This document serves as an update to the Dec. 18 version, which focused on sales tax changes implemented between 2010 and 2012. We now focus on changes implemented between 2009 and 2013. My comments have been updated to reflect the changed results.

## Distribution of Reforms
```{r load_reforms, include=FALSE}

reforms <- fread("C:/Users/John Bonney/Desktop/Magne_projects/sales_taxes/output/tr_events_comprehensive.csv")
state_fips <- fread("C:/Users/John Bonney/Desktop/Magne_projects/sales_taxes/data/state_fips_master.csv")
reforms <- merge(reforms, state_fips[, .(state_name, fips_state)],
                 by = "fips_state")
reforms[, ref_quarter := ceiling(ref_month / 3)]
reforms$year_qtr <- as.yearqtr(paste(as.integer(reforms$ref_year),
                                     as.integer(reforms$ref_quarter)), "%Y %q")
reforms[, qtr_total_increase := sum(change == "Increase"), by = "year_qtr"]
reforms[, qtr_total_decrease := sum(change == "Decrease"), by = "year_qtr"]
```

### When are reforms happening?
```{r plot_distribution, echo=FALSE, fig.cap = "\\label{fig:plot_dist}Distribution of Sales Tax Changes over Time"}
increases <- ggplot(data = reforms[change == "Increase"], mapping = aes(x = year_qtr)) +
               geom_histogram(binwidth = .25) +
               scale_x_yearqtr(format = "%Y-%q", n = 4) +
               labs(x = "Quarter", y = "# of sales tax increases") +
               theme_bw()
decreases <- ggplot(data = reforms[change == "Decrease"], mapping = aes(x = year_qtr)) +
               geom_histogram(binwidth = .25) +
               scale_x_yearqtr(format = "%Y-%q", n = 4) +
               labs(x = "Quarter", y = "# of sales tax decreases") +
               theme_bw()
plot_grid(increases, decreases)
```

As we can see in Figure \ref{fig:plot_dist}, we have five "spikes" of more than 50 sales tax increases occuring in one quarter and two "spikes" of similar decreases. Are these spikes individual state-level reforms, or are they different reforms that happen to coincide?

```{r find_spikes_increase, echo=TRUE}

# Which states are driving the spikes in increases?

with(reforms[change == "Increase" & qtr_total_increase >= 50],
     table(state_name, year_qtr))
```

The spikes in tax increases appear to be driven by eleven different state-level reforms (as well as some county-specific reforms in other states) in Arkansas, California (twice), Kansas, Massachusetts, Minnesota, Nevada, New Mexico, North Carolina, Ohio, and Virginia.

```{r find_spikes_decrease, echo = TRUE}
# Which states are driving the spike in decreases?

with(reforms[change == "Decrease" & qtr_total_decrease >= 50],
     table(state_name, year_qtr))
```

```{r pinpoint_changes, eval=FALSE, include=FALSE}
changes <- fread("C:/Users/John Bonney/Dropbox/Sales tax/Data/tax_reforms_all.csv")
# increases
changes[ref_year == 2010 & fips_state == 35] # New Mexico
changes[ref_year == 2010 & fips_state == 20] # Kansas

# decreases
changes[ref_year == 2011 & fips_state == 37] # North Carolina
changes[ref_year == 2011 & fips_state == 6]  # California
changes[ref_year == 2013 & fips_state == 20] # Kansas

# past increases for decreasing states
changes[ref_year == 2009 & fips_state == 37]
changes[ref_year == 2009 & fips_state == 6]
changes[ref_year == 2010 & fips_state == 20]


```

The spikes in tax decreases were specifically driven by three reforms:

* In July 2011, North Carolina **decreased** its state sales tax by 1.0%, from 5.75% to 4.75% *(100 counties)*
* In July 2011, California **decreased** its state sales tax by 1.0%, from 8.25% to 7.25% *(58 counties)*
* In July 2013, Kansas **decreased** its state sales tax by 0.15%, from 6.3% to 6.15% *(105 counties)*

Note that North Carolina and California had increased sales taxes in 2009 and Kansas had increased sales taxes in 2010 (all three of these increases were by 1.0%). Consequently, these decreases aren't abrupt changes in a long-standing policy, but rather a full or partial reversal of a recent policy change.

### Where are reforms happening?

![\label{fig:map}Geographical Distribution of Sales Tax Changes](C:/Users/tax_compr_2009-2013.png)

<!-- C:/Users/John Bonney/Desktop/Magne_projects/sales_taxes/output/figs/maps/tax_compr.png -->

The only state-wide tax changes that appear in this map (Figure \ref{fig:map}) but are not captured in the previous analysis are:

* Arizona's June 2010 reform, which **increased** state sales tax by 1%, from 5.6% to 6.6%
* Arizona's June 2013 reform, which **decreased** state sales tax by 1%, from 6.6% to 5.6%
* Connecticut's July 2011 reform, which **increased** state sales tax by 0.35%, from 6.0% to 6.35%
* Maine's October 2013 reform, which **increased** state sales tax by 0.5%, from 5.0% to 5.5%

```{r merge_covariates, include = FALSE}
## Calculate the number of increasers/decreasers/total counties by state
county_changes <- reforms
county_changes[, ever_increase := max(as.integer(change == "Increase")),
              by = .(fips_state, fips_county)]
county_changes[, ever_decrease := max(as.integer(change == "Decrease")),
              by = .(fips_state, fips_county)]
county_changes <- unique(county_changes[, .(fips_state, fips_county,
                                          ever_increase, ever_decrease)])

dist_by_state <- county_changes[, list(increasers = sum(ever_increase),
                                      decreasers = sum(ever_decrease)),
                               by = fips_state]

## This is a little convoluted since the reforms dataset only has state fips
## codes, so I need to match those up to real names (as well as total # of
## counties in each state)

counties <- fread("C:/Users/John Bonney/Desktop/Magne_projects/sales_taxes/data/us_counties_from_wikipedia.csv")
counties <- counties[, .(n_counties = .N), by = State]
setnames(counties, old = "State", new = "state_name")
# setnames(state_fips, old = "fips", new = "fips_state")
counties <- merge(counties,
                  state_fips[, .(state_name, fips_state, state_abbr)],
                  by = "state_name")

dist_by_state <- merge(dist_by_state, counties, by = "fips_state", all = T)
dist_by_state[is.na(increasers), increasers := 0]
dist_by_state[is.na(decreasers), decreasers := 0]
setcolorder(dist_by_state, c("fips_state", "state_name", "state_abbr",
                             "increasers", "decreasers", "n_counties"))
setnames(dist_by_state,
         old = c("state_name", "increasers", "decreasers", "n_counties"),
         new = c("State", "# ever increase", "# ever decrease",
                 "Total"))
```

```{r table_state_changes, echo = FALSE}
kable(dist_by_state[, .(State, `# ever increase`, `# ever decrease`, Total)],
                booktabs = T,
                caption = "Number of counties that increased/decreased sales taxes between 2009 and 2013 (by state)")
```

I show a breakdown of all county-level changes, state-by-state, in Table 1.

## Predictive characteristics
    
```{r clean_covariates, include = FALSE}
main_dir <- "C:/Users/John Bonney/Desktop/Magne_projects/sales_taxes/data/"
zillow_path <- paste0(main_dir, "zillow/zillow_long_by_county_clean.csv")
nhgis_path <- paste0(main_dir, "nhgis/nhgis_county_clean.csv")
qcew_path <- paste0(main_dir, "bls_qcew/qcew_clean.csv")
unemp_path <- paste0(main_dir, "bls_cty_unemp/county_unemployment_clean.csv")

tr_groups <- fread(
  "C:/Users/John Bonney/Desktop/Magne_projects/sales_taxes/output/tr_groups_comprehensive.csv"
  )[, .(fips_county, fips_state, tr_group)]

## create dummies for treatment groups
tr_groups[, ever_increase := max(tr_group == "Ever increase"),
          by = .(fips_state, fips_county)]
tr_groups[, ever_decrease := max(tr_group == "Ever decrease"),
          by = .(fips_state, fips_county)]
tr_groups <- unique(tr_groups[, .(fips_state, fips_county,
                                  ever_increase, ever_decrease)])

## read in covariates
zillow_data <- fread(zillow_path)[year == 2006 & as.integer(round(month)) == 1]
nhgis_data <- fread(nhgis_path)[year == 2000]
setnames(nhgis_data, old = c("statefp", "countyfp"),
                     new = c("fips_state", "fips_county"))
qcew_data <- fread(qcew_path)[year == 2006]
qcew_data[, fips_state := as.integer(substring(area_fips, 0, 2))]
qcew_data[, fips_county := as.integer(substring(area_fips, 3, 5))]
qcew_data[, c("V1", "area_fips", "year") := NULL]
unemp_data <- fread(unemp_path)[year == 2006]
setnames(unemp_data, old = c("statefp", "countyfp"),
                     new = c("fips_state", "fips_county"))

## merge on covariates
tr_groups <- merge(tr_groups, zillow_data[, .(fips_state, fips_county,
                                              SizeRank, median_home_price)],
                   by = c("fips_state", "fips_county"),
                   all.x = T)
tr_groups <- merge(tr_groups,
                   nhgis_data[, .(fips_state, fips_county, pop, pop_urban,
                                  pop_black, pop_over_65, pop_under_25,
                                  pct_pop_over_65, pct_pop_under_25,
                                  pct_pop_black, pct_pop_urban,
                                  pct_pop_no_college, pct_pop_bachelors,
                                  median_income, total_housing,
                                  owner_occupied_housing,
                                  housing_ownership_share)],
                   by = c("fips_state", "fips_county"),
                   all.x = T)
tr_groups <- merge(tr_groups, qcew_data, by = c("fips_state", "fips_county"))
tr_groups <- merge(tr_groups, unemp_data[, .(fips_state, fips_county,
                                             laborforce, employed,
                                             unemployed, unemp_rate)],
                   by = c("fips_state", "fips_county"),
                   all.x = T)
setnames(tr_groups, old = "total_employment", new = "annual_avg_emplvl")
tr_groups[, ln_pop := log(pop)]
tr_groups[, ln_median_income := log(median_income)]
tr_groups[, ln_median_home_price := log(median_home_price)]
tr_groups[, pct_establishments_retail := retail_establishments / total_establishments]
tr_groups[, ln_retail_mean_wage := log(retail_mean_wage)]

tr_groups[, pct_pop_over_65 := pct_pop_over_65 * 100]
tr_groups[, pct_pop_under_25 := pct_pop_under_25 * 100]
tr_groups[, pct_pop_black := pct_pop_black * 100]
tr_groups[, pct_pop_urban := pct_pop_urban * 100]
tr_groups[, pct_pop_no_college := pct_pop_no_college * 100]
tr_groups[, pct_pop_bachelors := pct_pop_bachelors * 100]
tr_groups[, housing_ownership_share := housing_ownership_share * 100]
tr_groups[, construction_empshare := construction_empshare * 100]
tr_groups[, finance_insurance_empshare := finance_insurance_empshare * 100]
tr_groups[, manufacturing_empshare := manufacturing_empshare * 100]
tr_groups[, public_admin_empshare := public_admin_empshare * 100]
tr_groups[, realestate_empshare := realestate_empshare * 100]
tr_groups[, retail_empshare := retail_empshare * 100]
tr_groups[, food_and_drugstores_empshare := food_and_drugstores_empshare * 100]
tr_groups[, pct_establishments_retail := pct_establishments_retail * 100]

tr_groups[, newmexico := as.integer(fips_state == 35)]
tr_groups[, kansas := as.integer(fips_state == 20)]
tr_groups[, northcarolina := as.integer(fips_state == 37)]
tr_groups[, california := as.integer(fips_state == 6)]

## Data is all cleaned up and merged now
## Note that there are two different measures of employment (see https://www.bls.gov/lau/laumthd.htm)
```
We have some variables obtained from Zillow.com and IPUMS National Historical Geographic Information System (NHGIS) in addition to the Quarterly Census of Employment and Wages (QCEW) and the Local Area Unemployment Statistics, both from the Bureau of Labor Statistics (see Table 2). I use these data (restricted to information determined prior to the tax change) to see which county characteristics "predict" sales tax changes.
           
```{r table_covariates, echo = FALSE}
var_descriptions <- data.table(
  Variable = c("$\\texttt{SizeRank}$", "$\\texttt{median\\_home\\_price}$", "$\\texttt{pop}$",
               "$\\texttt{pop\\_urban}$", "$\\texttt{pop\\_black}$", "$\\texttt{pop\\_over\\_65}$",
               "$\\texttt{pop\\_under\\_25}$", "$\\texttt{pct\\_pop\\_no\\_college}$",
               "$\\texttt{pct\\_pop\\_bachelors}$", "$\\texttt{median\\_income}$",
               "$\\texttt{total\\_housing}$", "$\\texttt{owner\\_occupied\\_housing}$",
               "$\\texttt{annual\\_avg\\_emplvl}^{\\text{*}}$", "$\\texttt{retail\\_employment}$",
               "$\\texttt{total\\_establishments}$", "$\\texttt{retail\\_establishments}$",
               "$\\texttt{food\\_and\\_drugstores\\_empshare}$", "$\\texttt{total\\_mean\\_wage}$",
               "$\\texttt{retail\\_mean\\_wage}$", "$\\texttt{laborforce}$", "$\\texttt{employed}^{\\text{*}}$",
               "$\\texttt{unemployed}$", "$\\texttt{unemp\\_rate}$"),
  Source = c("Zillow", "Zillow", "NHGIS", "NHGIS", "NHGIS", "NHGIS", "NHGIS", "NHGIS",
             "NHGIS", "NHGIS", "NHGIS", "NHGIS", "QCEW", "QCEW", "QCEW", "QCEW", "QCEW",
             "QCEW", "QCEW", "LAUS", "LAUS", "LAUS", "LAUS"),
  Description = c("Rank of county size (by pop.) (Jan. 2006)",
                  "Median home price (Jan. 2006)",
                  "County pop. (2000)",
                  "Pop. living in urban areas (2000)",
                  "Pop. identifying as black (2000)",
                  "Pop. over age 65 (2000)",
                  "Pop. under age 25 (2000)",
                  "Pct. pop. without a college degree (2000)",
                  "Pct. pop. with bachelor's degree or higher (2000)",
                  "Median income (2000)",
                  "Total housing units (2000)",
                  "Total owner-occupied housing units (2000)",
                  "Establishment employment level (2006)",
                  "Retail industry employment (2006)",
                  "Total number of establishments (2006)",
                  "Total number of retail establishments (2006)",
                  "Fraction of workers in food and drugstores (2006)",
                  "Mean wage (2006)",
                  "Mean wage in retail industry (2006)",
                  "Labor force size (2006)",
                  "Total employment level (2006)",
                  "Unemployed level (2006)",
                  "Unemployment rate (2006)")
)

kable(var_descriptions[, .(Variable, Source, Description)],
                booktabs = T,
                caption = "Summary of variables", escape = F) %>%
  footnote(symbol = "Note that there are two measures of employment, one from the QCEW and one from the LAUS. This is because the QCEW estimates employment levels using establishment data, but the BLS adjusts these measures from a place-of-work basis to a place-of-residence basis using commutation factors calculated from the ACS. For more information, see the \\\\href{https://www.bls.gov/lau/laumthd.htm}{\\\\underline{BLS estimation methodology}}.",
           threeparttable = T,
           escape = F)
```

I adapted many of the variables in Table 2 to be in percentage or log terms for comparability across counties. In addition to these variables, we have industry employment shares for a number of different industries from the QCEW.

To determine what features are indicative of a future sales tax change, I run predictive regressions similar to those run by Deshpande and Li (2018). I estimate three different models for each type of sales tax reform (increase and decrease). The first model includes population-specific covariates relating to housing, employment, income, race, and education. The second model includes industry employment shares for the county for six specific industries. The third model combines the covariates from the first two models, and is as follows:

$$
y_{c} = \beta PopChar_{c2000} + \gamma Housing_{c2000,2006} + \delta LaborForce_{c2006} + \kappa EmpShare_{c2006} + \epsilon_{c},
$$

```{r predictive_regressions, include=FALSE}

# TODO: work on the aesthetics here

## Run predictive Manasi-style regression
baseline_covariates <- paste0(
  "ln_pop + unemp_rate + ln_median_home_price + pct_pop_urban + pct_pop_black + ",
  "pct_pop_over_65 + pct_pop_under_25 + pct_pop_no_college + ln_median_income + ",
  "housing_ownership_share"
  )

industry_shares <- paste0(
  "retail_empshare + construction_empshare + finance_insurance_empshare + ",
  "manufacturing_empshare + public_admin_empshare + realestate_empshare"
)

baseline_formula <- as.formula(paste0("ever_increase ~ ", baseline_covariates))
baseline_model <- glm(formula = baseline_formula,
                      data = tr_groups[ever_decrease == 0 | ever_increase == 1],
                      family = "binomial")
res1 <- cbind(summary(baseline_model)$coefficients,
              rep(1, nrow(summary(baseline_model)$coefficients)))

ind_shares_formula <- as.formula(paste0("ever_increase ~ ", industry_shares))
model2 <- glm(formula = ind_shares_formula,
              data = tr_groups[ever_decrease == 0 | ever_increase == 1],
              family = "binomial")
res2 <- cbind(summary(model2)$coefficients,
              rep(2, nrow(summary(model2)$coefficients)))

full_formula <- as.formula(paste0("ever_increase ~ ",
                                  baseline_covariates, "+",
                                  industry_shares))
model3 <- glm(formula = full_formula,
              data = tr_groups[ever_decrease == 0 | ever_increase == 1],
              family = "binomial")
res3 <- cbind(summary(model3)$coefficients,
              rep(3, nrow(summary(model3)$coefficients)))

increase_res <- rbind(res1, res2, res3)
increase_res <- data.table(variable = rownames(increase_res),
                           coef = increase_res[, "Estimate"],
                           std_err = increase_res[, "Std. Error"],
                           z_val = increase_res[, "z value"],
                           p_val = increase_res[, "Pr(>|z|)"],
                           model = increase_res[, 5])
increase_res[, coef_ws := paste0(round(coef, 3),
                        ifelse(p_val < 0.01, "***",
                        ifelse(p_val < 0.05, "**",
                        ifelse(p_val < 0.10, "*", ""))))]
increase_res[, std_err_display := paste0("(", round(std_err, 3), ")")]

N_obs_i1 <- nobs(baseline_model)
N_obs_i2 <- nobs(model2)
N_obs_i3 <- nobs(model3)

## Interesting -- those with higher percent retail employment share were less likely to increase sales taxes

## Now let's look at decreases
baseline_formula <- as.formula(paste0("ever_decrease ~ ", baseline_covariates))
baseline_model <- glm(formula = baseline_formula,
                      data = tr_groups[ever_decrease == 1 | ever_increase == 0],
                      family = "binomial")
res1 <- cbind(summary(baseline_model)$coefficients,
              rep(1, nrow(summary(baseline_model)$coefficients)))

ind_shares_formula <- as.formula(paste0("ever_decrease ~ ", industry_shares))
model2 <- glm(formula = ind_shares_formula,
              data = tr_groups[ever_decrease == 1 | ever_increase == 0],
              family = "binomial")
res2 <- cbind(summary(model2)$coefficients,
              rep(2, nrow(summary(model2)$coefficients)))

full_formula <- as.formula(paste0("ever_decrease ~ ",
                                  baseline_covariates, "+",
                                  industry_shares))
model3 <- glm(formula = full_formula,
              data = tr_groups[ever_decrease == 1 | ever_increase == 0],
              family = "binomial")
res3 <- cbind(summary(model3)$coefficients,
              rep(3, nrow(summary(model3)$coefficients)))

decrease_res <- rbind(res1, res2, res3)
decrease_res <- data.table(variable = rownames(decrease_res),
                           coef = decrease_res[, "Estimate"],
                           std_err = decrease_res[, "Std. Error"],
                           z_val = decrease_res[, "z value"],
                           p_val = decrease_res[, "Pr(>|z|)"],
                           model = decrease_res[, 5])
decrease_res[, coef_ws := paste0(round(coef, 3),
                        ifelse(p_val < 0.01, "***",
                        ifelse(p_val < 0.05, "**",
                        ifelse(p_val < 0.10, "*", ""))))]
decrease_res[, std_err_display := paste0("(", round(std_err, 3), ")")]

N_obs_d1 <- nobs(baseline_model)
N_obs_d2 <- nobs(model2)
N_obs_d3 <- nobs(model3)
## Hmm, things are suspiciously significant...

######## FORMATTING ###########
decrease_res <- decrease_res[variable != "(Intercept)"]
decrease_res[, order := 1:nrow(decrease_res)]

order_dt <- decrease_res[model == 3, .(variable, order)]
order_dt[, order := order - 16]

decrease_long <- rbind(decrease_res[, .(variable, coef_ws, std_err_display, order, model)],
                       decrease_res[, .(variable, coef_ws, std_err_display, order, model)])
setorder(decrease_long, order)
decrease_long[, type := rep(c("coef", "std_err"), nrow(decrease_res))]
decrease_long[, display := ifelse(type == "coef",
                                  coef_ws,
                                  std_err_display)]
decrease_long <- decrease_long[, .(variable, type, model, display, order)]
setorder(decrease_long, order, type)

master_res <- rbind(order_dt, order_dt)
master_res[, type := rep(c("coef", "std_err"), each = nrow(order_dt))]

for (model_type in 1:3){
  eval(parse(text = paste0(
    "decrease_long_", model_type, "<- decrease_long[model == model_type]"
  )))
  eval(parse(text = paste0(
    "setnames(decrease_long_", model_type, ", old = 'display', new = 'display_decr_", model_type, "')"
  )))
  eval(parse(text = paste0(
    "decrease_long_", model_type, "[, c('model', 'order') := NULL]"
  )))
  eval(parse(text = paste0(
    "master_res <- merge(master_res, decrease_long_", model_type,
    ", all.x = T, by = c('variable', 'type'))"
  )))
}

increase_res <- increase_res[variable != "(Intercept)"]
increase_res[, order := 1:nrow(increase_res)]

increase_long <- rbind(increase_res[, .(variable, coef_ws, std_err_display, order, model)],
                       increase_res[, .(variable, coef_ws, std_err_display, order, model)])
setorder(increase_long, order)
increase_long[, type := rep(c("coef", "std_err"), nrow(increase_res))]
increase_long[, display := ifelse(type == "coef",
                                  coef_ws,
                                  std_err_display)]
increase_long <- increase_long[, .(variable, type, model, display, order)]
setorder(increase_long, order, type)

for (model_type in 1:3){
  eval(parse(text = paste0(
    "increase_long_", model_type, "<- increase_long[model == model_type]"
  )))
  eval(parse(text = paste0(
    "setnames(increase_long_", model_type, ", old = 'display', new = 'display_incr_", model_type, "')"
  )))
  eval(parse(text = paste0(
    "increase_long_", model_type, "[, c('model', 'order') := NULL]"
  )))
  eval(parse(text = paste0(
    "master_res <- merge(master_res, increase_long_", model_type,
    ", all.x = T, by = c('variable', 'type'))"
  )))
}

master_res <- rbind(master_res,
                    data.table(variable = "Obs.",
                               type = "",
                               order = nrow(order_dt) + 1,
                               display_decr_1 = N_obs_d1,
                               display_decr_2 = N_obs_d2,
                               display_decr_3 = N_obs_d3,
                               display_incr_1 = N_obs_i1,
                               display_incr_2 = N_obs_i2,
                               display_incr_3 = N_obs_i3))
master_res[type == "std_err", variable := ""]
nice_names <- data.table(nice_names = c(
  "Log(pop.)", "Unemp. rate", "Log(median home price)", "Urbanization rate",
              "Pct. black", "Pct. over 65", "Pct. under 25", "Pct. no college",
              "Log(median income)", "Housing ownership rate", "Retail emp. share",
              "Construction emp. share", "Finance/insurance emp. share",
              "Manufacturing emp. share", "Public admin. emp. share", "Real estate emp. share", "Obs."
  ),
  variable = c(
    "ln_pop", "unemp_rate", "ln_median_home_price", "pct_pop_urban", "pct_pop_black", "pct_pop_over_65",
    "pct_pop_under_25", "pct_pop_no_college", "ln_median_income", "housing_ownership_share", "retail_empshare",
    "construction_empshare", "finance_insurance_empshare", "manufacturing_empshare", "public_admin_empshare",
    "realestate_empshare", "Obs."))
master_res <- merge(master_res, nice_names, all.x = T)
setorder(master_res, order, type)
master_res[is.na(nice_names), nice_names := ""]
master_res[is.na(display_decr_1), display_decr_1 := ""]
master_res[is.na(display_decr_2), display_decr_2 := ""]
master_res[is.na(display_decr_3), display_decr_3 := ""]
master_res[is.na(display_incr_1), display_incr_1 := ""]
master_res[is.na(display_incr_2), display_incr_2 := ""]
master_res[is.na(display_incr_3), display_incr_3 := ""]

```
```{r table_predictive_res, echo=FALSE}
kable(master_res[, .(nice_names, display_decr_1, display_decr_2, display_decr_3,
                     display_incr_1, display_incr_2, display_incr_3)],
      format = "latex", booktabs = T, col.names = NULL,
      caption = "Predictive Characteristics") %>%
  add_header_above(c(" ", "Ever decrease" = 3, "Ever increase" = 3)) %>%
  row_spec(32, hline_after = T) %>%
  footnote(general = '*** p<0.01, ** p<0.05, * p<0.1. Percentages and fractions were multiplied by 100, so coefficients represent a 1 pp increase. Estimation is by logistic regression. Increase only counties are excluded from the "ever increase" estimation and vice versa for the "ever decrease" estimation. Standard errors in parentheses.',
           threeparttable = T)

```

where $PopChar_{c2000}$ is a vector of population characteristics; $Housing_{c2000,2006}$ is a vector of housing characteristics, including the local median home price in 2006 and the urbanization and home ownership rates in 2000; $LaborForce_{c2006}$ is a vector of local labor force characteristics in 2006; $EmpShare_{c2006}$ is a vector of industry employment shares; and $\epsilon_c$ encapsulates all other factors impacting any county's decision to change sales taxes. Note that the first specification excludes $EmpShare_{c2006}$, while the second specification excludes all other variables.

Results are found in Table 3. We see that higher populations, higher unemployment rates, a more educated population, lower incomes, and higher home prices are associated with a higher likelihood of *decreasing* sales taxes; a more educated population, lower incomes, less retail employment, and less manufacturing are associated with a higher likelihood of *increasing* sales taxes.

Interestingly, when comparing the results between the "ever decrease" and "ever increase" groups, the signs of the coefficients on a majority of the predictive characteristics are the same: percent of the population with no college (negative), log median income (negative), and finance/insurance employment share (negative). This indicates that these counties which are increasing or decreasing sales tax rates are different from the counties that do not change their sales tax rates, but different in a similar way.

When interpreting these results, it is important to remember that the states and time periods mentioned in the **Distribution of Reforms** section make up a significant portion of the treatment groups, so some results may be reflecting characteristics of those states or those time periods that may not be relevant for tax reforms. As a check, I re-run the regressiosn with dummies for California and North Carolina ("ever decrease") and New Mexico and Kansas ("ever increase"). Coefficients on these dummies are insignificant (z-scores very close to 0). Most of the previously significant factors remain significant; however, all coefficients exhibit significant shrinkage. These results are available upon request.

```{r add_state_dummies, eval=FALSE, include=FALSE}
robustness_formula1 <- as.formula(paste0("ever_increase ~ ",
                                  baseline_covariates, "+",
                                  industry_shares,
                                  " + newmexico + kansas"))
robustness_check1 <- glm(formula = robustness_formula1,
              data = tr_groups[ever_decrease == 0 | ever_increase == 1],
              family = "binomial")
summary(robustness_check1)

robustness_formula2 <- as.formula(paste0("ever_increase ~ ",
                                  baseline_covariates, "+",
                                  industry_shares,
                                  " + california + northcarolina"))
robustness_check2 <- glm(formula = robustness_formula2,
              data = tr_groups[ever_decrease == 0 | ever_increase == 1],
              family = "binomial")
summary(robustness_check2)


```
# Seasonality

## Product-specific seasonality

The goal here is to identify which goods exhibit the most seasonality over time. I do this on the server due to the size of the dataset. I estimate the following model on the product-store-quarter-year ($psqy$) level:[^3] $$ y_{psqy} = \alpha_{qp} + \gamma_ptime_{t(q,y)} + \epsilon_{psqy},$$ where $$y_{psqy} = \textrm{ln}\left(\frac{sales_{psqy}}{sales_{psQ_12008}}\right),$$ $\gamma_p$ is a product-specific linear time effect, and $\alpha_{qp}$ is a product-quarter fixed effect. I then calculate a seasonality range for each product, $$SR_p = \max_{q}(\alpha_{qp})-\min_{q}(\alpha_{qp}).$$ $SR_p$ is then essentially the product-specific ratio of normalized log sales of the season with the highest average sales (peak-season) to the season with the lowest average sales (low-season).[^4] Taking $e^{SR_p}$ gives a rough estimate of the actual ratio of peak-season sales to low-season sales.[^5] We use $SR_p$ primarily as an index to compare seasonality across goods.

[^3]: For computational purposes, I instead residualize out the linear time trend to obtain $\widetilde{y}_{psqy} = y_{psqy} - \gamma_ptime_{t(q,y)}$ and then take the mean of $\widetilde{y}_{psqy}$ over all stores $s$ and years $y$ for each product-quarter pair to obtain $\alpha_{qp}.$ This is functionally equivalent to what the estimated product-quarter fixed effects would be (without an intercept).

[^4]: Consider this measure restricted to only one store $s$ in just one year, $y=2008.$ See that $$\alpha_{qp} = \ln{\frac{sales_{pq}}{sales_{pQ_1}}}.$$ Then we have
    $$
    \begin{aligned}
    SR_p &= \max_{q}(\alpha_{qp})-\min_{q}(\alpha_{qp}) \\
    &= \max_{q}\biggl(\ln{\frac{sales_{pq}}{sales_{pQ_1}}}\biggr) - \min_{q}\biggl(\ln{\frac{sales_{pq}}{sales_{pQ_1}}}\biggr) \\
    &= \ln{\left(\frac{\max_q{(sales_{pq}/sales_{pQ_1}})}{\min_q{(sales_{pq}/sales_{pQ_1}})}\right)} \\
    &= \ln{\left(\frac{\max_q{sales_{pq}}}{\min_q{sales_{pq}}}\right)}
    \end{aligned}
    $$
    We can extend this to an arbitrary number of stores and years (assuming a balanced panel for each product). Let $\bar{q}$ denote the quarter that maximizes $\alpha_{qp},$ let $\underline{q}$ denote the quarter that minimizes $\alpha_{qp},$ and let $N_{s,y}$ denote the number of store-years in the data. It is simple to show that
    $$
    SR_p = \frac{1}{N_{s,y}}\sum_{s,y}\ln\biggl(\frac{sales_{ps\bar{q}y}}{sales_{ps\underline{q}y}}\biggr),
    $$
    which is clearly the average over all store-years of the log ratio of sales in $\bar{q}$ to sales in $\underline{q}.$

[^5]: Continuing notation from footnote 4, we have $$ e^{SR_p} = \biggl(\prod_{s,y}\frac{sales_{ps\bar{q}y}}{sales_{ps\underline{q}y}}\biggr)^{\frac{1}{N_{s,y}}}$$
    
We can use these resulting $SR_p$ indicators to examine the distribution of seasonality within the 265 best-selling products left in the balanced panel.

```{r plot_seasonality, echo = FALSE, fig.cap = "\\label{fig:fig3}Distribution of Seasonality Range across Products"}
seasonality <- fread("C:/Users/John Bonney/Desktop/Magne_projects/sales_taxes/output/server/product_seasonality.csv")
ggplot(data = seasonality, mapping = aes(x = seasonality_range)) +
  geom_histogram(bins = 70) +
  theme_bw() +
  labs(x = expression(italic(SR["p"])), y = "Number of products")
```

Most products appear to exhibit some sort of seasonality (Figure \ref{fig:fig3}). However, seasonality across products exhibits considerable heterogeneity, and there are clearly some outliers.

The most seasonal goods by far are specialty chocolates, electric fans, and sunscreens/sunblocks (see Table 4). Not surprisingly, these are goods one would expect to be correlated with seasons. Prices of fans and sun products peak as summer begins, in $\text{Q}_2,$ and reach their lowest point as winter begins, in $\text{Q}_4.$ Specialty chocolates have highest sales during the quarter that contains Valentine's day.

This serious seasonality is only exhibited for 3 out of the 265 products we examine, and is thus unlikely to be the driver of seasonality issues in the data. However, it is worth noting that items with more seasonal sales seem more likely to be non-food items (and thus taxable), while items with less seasonal sales are food items.


```{r table_seasonality, echo = FALSE}
seasonality[, module_description := gsub("&", "\\\\&", module_description)]
seasonality[, module_description := paste0(substr(module_description, 1, 1),
                                           tolower(substr(module_description, 2, nchar(module_description))))]
setorder(seasonality, -seasonality_range)
seasonality[, exp_SR_p := exp(seasonality_range)]
kable(seasonality[c(1:15, 251:265), .(module_description, seasonality_range, exp_SR_p, min_season_type, max_season_type)],
                booktabs = T,
                caption = "Products with highest/lowest seasonality range",
      col.names = c("Module description", "$SR_p$", "$e^{SR_p}$", "Low season", "Peak season"),
      escape = F,
      digits = 2) %>%
  kable_styling() %>%
  group_rows("Highest $SR_p$", 1, 15, escape = F) %>%
  group_rows("Lowest $SR_p$", 16, 30, escape = F)
```

## County-specific seasonality

I also estimate county-specific seasonality, using the same method I used to estimate product-specific seasonality (but replacing product-level season effects with county-level season effects). Note that this implicitly weights all store-product level sales equally within each county and does not account for interactions between product and county seasonality. I denote the resulting seasonality range $SR_c.$

```{r plot_seasonality_county, echo = FALSE, fig.cap = "\\label{fig:fig4}Distribution of Seasonality Range across Counties"}
cty_seasonality <- fread("C:/Users/John Bonney/Desktop/Magne_projects/sales_taxes/output/server/county_seasonality.csv")
ggplot(data = cty_seasonality, mapping = aes(x = seasonality_range)) +
  geom_histogram(bins = 70) +
  theme_bw() +
  labs(x = expression(italic(SR["c"])), y = "Number of counties")
```

We can see in (Figure \ref{fig:fig4}) that county-specific seasonality follows a similar pattern as product-specific seasonality, but with a shorter right tail (note the differences in the x-axis scale between Figure \ref{fig:fig3} and Figure \ref{fig:fig4}). There are some outliers on the right tail of the distribution.

A t-test shows that the mean populations of counties with $SR_c < 0.3$ and those with $SR_c >= 0.3$ are significantly different (t-stat = 6.1) - counties with abnormally large $SR_c$ are on average smaller counties.[^6]

[^6]: These results hold when population is replaced with an ordered population-rank variable (t-stat = -3.5)

```{r table_seasonality_county, echo = FALSE}
county_names <- fread('C:/Users/John Bonney/Desktop/Magne_projects/sales_taxes/data/county_fips_master.csv')
county_names <- county_names[, .(fips, long_name)]
cty_seasonality <- merge(cty_seasonality, county_names, by = "fips")

setorder(cty_seasonality, -seasonality_range)
cty_seasonality[, exp_SR_p := exp(seasonality_range)]
kable(cty_seasonality[c(1:15, 251:265), .(long_name, seasonality_range, exp_SR_p, min_season_type, max_season_type)],
                booktabs = T,
                caption = "Counties with highest/lowest seasonality range",
      col.names = c("County", "$SR_p$", "$e^{SR_p}$", "Low season", "Peak season"),
      escape = F,
      digits = 2) %>%
  kable_styling() %>%
  group_rows("Highest $SR_p$", 1, 15, escape = F) %>%
  group_rows("Lowest $SR_p$", 16, 30, escape = F)
```

```{r explore_county_seasonality, eval=FALSE, include=FALSE}
county_pop <- fread("C:/Users/John Bonney/Dropbox/Sales tax/Data/county_population.csv")
county_pop[, fips := fips_state * 1000 + fips_county]
cty_seasonality <- merge(cty_seasonality, county_pop, by = "fips")
setorder(cty_seasonality, -population)
cty_seasonality$poporder <- 1:nrow(cty_seasonality)
# Test difference in means of below and above .3 SR
t.test(cty_seasonality %>% filter(seasonality_range <= .3) %>% pull(population),
       cty_seasonality %>% filter(seasonality_range > .3) %>% pull(population))
t.test(cty_seasonality %>% filter(seasonality_range <= .3) %>% pull(poporder),
       cty_seasonality %>% filter(seasonality_range > .3) %>% pull(poporder))
setorder(cty_seasonality, -seasonality_range)
```

List of figures to include
-- Create quarterly figures for prices
-- Price graph -- compr posttax es resid A and D
-- Price graph -- calendar time A and D
-- Sales graph -- taxable only -- calendar time, A and D
-- Create raw figure for taxable only goods for sales
-- Sales graph -- taxable only -- event time, A and D
